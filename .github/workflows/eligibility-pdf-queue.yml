name: PDF Queue & Eligibility Check

on:
  repository_dispatch:
    types: [process-pdfs]
  workflow_dispatch:
    inputs:
      pdf_urls:
        description: 'JSON array of PDF URLs'
        required: true
        default: '[]'

permissions:
  contents: read
  actions: read

jobs:
  pdf-queue:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-hin \
            poppler-utils \
            libpoppler-cpp-dev

      - name: Install Python dependencies
        run: |
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements.txt

      - name: Parse webhook payload
        id: payload
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo '${{ toJson(github.event.client_payload.pdfs) }}' > pdfs.json
            MODE='${{ github.event.client_payload.mode }}'
            if [ -z "$MODE" ]; then MODE="auto"; fi
            echo "mode=$MODE" >> $GITHUB_OUTPUT
          else
            echo '${{ inputs.pdf_urls }}' > pdfs.json
            echo "mode=manual" >> $GITHUB_OUTPUT
          fi
          echo "=== PDF Payload ===" 
          cat pdfs.json | python3 -m json.tool 2>/dev/null | head -15 || echo "Invalid JSON"

      - name: Download PDFs
        run: |
          mkdir -p .cache tmp
          python3 << 'PYTHON_SCRIPT'
          import json, requests, pathlib, time, sys
          from urllib.parse import urlparse
          
          try:
              pdfs_data = json.load(open('pdfs.json'))
          except:
              pdfs_data = []
          
          download_log = []
          
          for i, pdf_info in enumerate(pdfs_data):
              url = pdf_info.get('url') if isinstance(pdf_info, dict) else None
              
              if not url:
                  continue
              
              filename = f".cache/pdf_{i:03d}_{pathlib.Path(urlparse(url).path).name[:30]}"
              
              try:
                  print(f"[{i+1}/{len(pdfs_data)}] Downloading: {url[:70]}...", file=sys.stderr)
                  
                  r = requests.get(
                      url,
                      headers={'User-Agent': 'Mozilla/5.0'},
                      timeout=20,
                      verify=True
                  )
                  r.raise_for_status()
                  
                  size = len(r.content) / 1024 / 1024
                  pathlib.Path(filename).write_bytes(r.content)
                  
                  download_log.append({
                      'url': url,
                      'filename': filename,
                      'size_mb': round(size, 2),
                      'status': 'success'
                  })
                  
                  print(f"  ✓ Downloaded ({size:.2f} MB)", file=sys.stderr)
                  time.sleep(0.5)
              
              except requests.exceptions.SSLError:
                  try:
                      print(f"  ⚠ SSL error, retrying...", file=sys.stderr)
                      r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20, verify=False)
                      r.raise_for_status()
                      pathlib.Path(filename).write_bytes(r.content)
                      download_log.append({'url': url, 'filename': filename, 'status': 'success_nossl'})
                      print(f"  ✓ Downloaded (no SSL)", file=sys.stderr)
                  except Exception as e2:
                      download_log.append({'url': url, 'status': 'failed', 'error': str(e2)[:80]})
                      print(f"  ✗ Failed: {e2}", file=sys.stderr)
              
              except requests.exceptions.Timeout:
                  download_log.append({'url': url, 'status': 'timeout'})
                  print(f"  ✗ Timeout (20s)", file=sys.stderr)
              
              except Exception as e:
                  download_log.append({'url': url, 'status': 'failed', 'error': str(e)[:80]})
                  print(f"  ✗ Failed: {e}", file=sys.stderr)
          
          pathlib.Path('tmp/download_log.json').write_text(json.dumps(download_log, indent=2))
          
          successful = sum(1 for x in download_log if x['status'].startswith('success'))
          print(f"\n✓ Downloaded {successful}/{len(pdfs_data)} PDFs", file=sys.stderr)
          PYTHON_SCRIPT

      - name: Save PDF queue for OCR workflow
        run: |
          # FIX P3-C-001: Pass PDF list to OCR workflow via artifact
          if [ -f tmp/download_log.json ]; then
            python3 << 'PYTHON_SCRIPT'
            import json, pathlib
            
            log = json.load(open('tmp/download_log.json'))
            downloaded = [x for x in log if x['status'].startswith('success')]
            
            # Create queue for OCR workflow
            ocr_queue = [
                {'pdf': x['filename'], 'url': x['url'], 'size_mb': x['size_mb']}
                for x in downloaded
            ]
            
            pathlib.Path('tmp/ocr_queue.json').write_text(
                json.dumps(ocr_queue, indent=2)
            )
            
            print(f"✓ Prepared {len(ocr_queue)} PDFs for OCR processing")
            PYTHON_SCRIPT
          fi

      - name: Upload PDFs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pdf-downloads
          path: tmp/download_log.json
          retention-days: 7

      - name: Trigger OCR workflow
        if: success()
        run: |
          # FIX P3-C-002: Pass pre-downloaded PDFs to OCR workflow
          echo "Triggering OCR workflow..."
          curl -X POST https://api.github.com/repos/${{ github.repository }}/dispatches \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -d '{"event_type":"ocr-start","client_payload":{"mode":"auto"}}' \
            -v || echo "⚠ OCR dispatch may have failed"

      - name: Cleanup
        if: always()
        run: |
          rm -rf .cache
          echo "✓ Cleanup complete"
