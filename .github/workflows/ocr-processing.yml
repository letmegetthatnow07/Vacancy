name: OCR Processing

on:
  repository_dispatch:
    types: [process-pdfs]
  workflow_dispatch:
    inputs:
      pdf_urls:
        description: 'JSON array of PDF URLs'
        required: true
        default: '[]'

permissions:
  contents: write

jobs:
  ocr-process:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-hin \
            poppler-utils \
            libpoppler-cpp-dev

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Parse webhook payload
        id: payload
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo '${{ toJson(github.event.client_payload.pdfs) }}' > pdfs.json
            MODE='${{ github.event.client_payload.mode }}'
            if [ -z "$MODE" ] || [ "$MODE" = "null" ]; then MODE="auto"; fi
          else
            echo '${{ inputs.pdf_urls }}' > pdfs.json
            MODE="manual"
          fi
          
          echo "mode=$MODE" >> $GITHUB_OUTPUT
          echo "=== PDF Payload ===" 
          cat pdfs.json | python3 -m json.tool | head -20
          echo "Mode: $MODE"

      - name: Download PDFs
        run: |
          mkdir -p .cache tmp
          python3 << 'PYTHON_SCRIPT'
          import json, requests, pathlib, time, sys
          from urllib.parse import urlparse
          
          pdfs_data = json.load(open('pdfs.json'))
          download_log = []
          
          for i, pdf_info in enumerate(pdfs_data):
              url = pdf_info.get('url')
              source = pdf_info.get('source', 'unknown')
              
              if not url:
                  continue
              
              # Generate safe filename
              filename = f".cache/pdf_{i:03d}_{pathlib.Path(urlparse(url).path).name[:30]}"
              
              try:
                  print(f"[{i+1}/{len(pdfs_data)}] Downloading: {url[:70]}...", file=sys.stderr)
                  
                  # FIX P2-C-004: Add per-PDF timeout + better error handling
                  r = requests.get(
                      url,
                      headers={'User-Agent': 'Mozilla/5.0'},
                      timeout=20,  # Per-file timeout
                      verify=True
                  )
                  r.raise_for_status()
                  
                  size = len(r.content) / 1024 / 1024  # MB
                  pathlib.Path(filename).write_bytes(r.content)
                  
                  download_log.append({
                      'url': url,
                      'filename': filename,
                      'size_mb': round(size, 2),
                      'status': 'success'
                  })
                  
                  print(f"  ✓ Downloaded ({size:.2f} MB)", file=sys.stderr)
                  time.sleep(0.3)
              
              except requests.exceptions.SSLError as e:
                  try:
                      print(f"  ⚠ SSL error, retrying without verification...", file=sys.stderr)
                      r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20, verify=False)
                      r.raise_for_status()
                      pathlib.Path(filename).write_bytes(r.content)
                      download_log.append({'url': url, 'filename': filename, 'status': 'success_nossl'})
                      print(f"  ✓ Downloaded (no SSL)", file=sys.stderr)
                  except Exception as e2:
                      download_log.append({'url': url, 'status': 'failed', 'error': str(e2)[:100]})
                      print(f"  ✗ Failed after retry: {e2}", file=sys.stderr)
              
              except requests.exceptions.Timeout:
                  download_log.append({'url': url, 'status': 'timeout'})
                  print(f"  ✗ Timeout (20s limit)", file=sys.stderr)
              
              except Exception as e:
                  download_log.append({'url': url, 'status': 'failed', 'error': str(e)[:100]})
                  print(f"  ✗ Failed: {e}", file=sys.stderr)
          
          # Write log
          with open('tmp/download_log.json', 'w') as f:
              json.dump(download_log, f, indent=2)
          
          successful = sum(1 for x in download_log if x['status'].startswith('success'))
          print(f"\n✓ Downloaded {successful}/{len(pdfs_data)} PDFs", file=sys.stderr)
          PYTHON_SCRIPT

      - name: Extract text with OCR
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json, subprocess, pathlib, sys, time
          from pathlib import Path
          
          pdfs = sorted(Path('.cache').glob('pdf_*'))
          results = []
          
          print(f"Starting OCR on {len(pdfs)} PDFs...", file=sys.stderr)
          
          for i, pdf_file in enumerate(pdfs):
              print(f"\n[{i+1}/{len(pdfs)}] Processing: {pdf_file.name}", file=sys.stderr)
              
              try:
                  # FIX: Use pdf_parser.py with deterministic IDs (Phase 1 compatible)
                  result = subprocess.run(
                      ['python3', 'tools/pdf_parser.py', str(pdf_file)],
                      capture_output=True,
                      text=True,
                      timeout=120  # 2 min per PDF
                  )
                  
                  if result.returncode == 0:
                      # FIX P2-H-001: Separate JSONL output from debug messages
                      # pdf_parser outputs JSONL to stdout, debug to stderr
                      for line in result.stdout.strip().split('\n'):
                          if line.strip() and line.startswith('{'):
                              try:
                                  job = json.loads(line)
                                  results.append(job)
                                  print(f"  ✓ Extracted: {job.get('title', 'N/A')[:50]}", file=sys.stderr)
                              except json.JSONDecodeError:
                                  pass
                      
                      if result.stderr:
                          print(f"  [log] {result.stderr[:150]}", file=sys.stderr)
                  else:
                      print(f"  ✗ OCR failed (code {result.returncode})", file=sys.stderr)
                      if result.stderr:
                          print(f"  Error: {result.stderr[:200]}", file=sys.stderr)
              
              except subprocess.TimeoutExpired:
                  print(f"  ✗ OCR timeout (120s)", file=sys.stderr)
              except Exception as e:
                  print(f"  ✗ Processing error: {e}", file=sys.stderr)
          
          # Write results
          pathlib.Path('tmp').mkdir(exist_ok=True)
          with open('tmp/ocr_results.jsonl', 'w', encoding='utf-8') as f:
              for job in results:
                  f.write(json.dumps(job, ensure_ascii=False) + '\n')
          
          print(f"\n✓ Extracted {len(results)} jobs from {len(pdfs)} PDFs", file=sys.stderr)
          PYTHON_SCRIPT

      - name: Merge OCR results (without overwriting applied jobs)
        run: |
          # FIX P2-C-002: DON'T run full schema_merge (already done by scraper)
          # Instead: manually merge OCR jobs to preserve applied_ids
          
          python3 << 'PYTHON_SCRIPT'
          import json, pathlib, sys
          from urllib.parse import urlparse
          import hashlib
          
          # Load existing data
          try:
              data = json.load(open('data.json'))
          except:
              print("[ERROR] data.json not found or invalid", file=sys.stderr)
              sys.exit(1)
          
          existing_jobs = data.get('jobListings', [])
          ocr_jobs = []
          
          # Load OCR results
          if pathlib.Path('tmp/ocr_results.jsonl').exists():
              for line in open('tmp/ocr_results.jsonl'):
                  if line.strip():
                      try:
                          ocr_jobs.append(json.loads(line))
                      except:
                          pass
          
          # FIX: Preserve applied_ids BEFORE merge
          applied_ids = set(data.get('sections', {}).get('applied', []))
          other_ids = set(data.get('sections', {}).get('other', []))
          
          print(f"[OCR] Preserving {len(applied_ids)} applied jobs", file=sys.stderr)
          
          # Merge OCR jobs without duplicating schema_merge logic
          # Just add new OCR jobs to existing list
          merged_count = 0
          for ocr_job in ocr_jobs:
              # Check if already exists by ID
              if not any(j['id'] == ocr_job['id'] for j in existing_jobs):
                  existing_jobs.append(ocr_job)
                  merged_count += 1
              else:
                  # Update existing job with OCR data
                  for j in existing_jobs:
                      if j['id'] == ocr_job['id']:
                          # Enrich with OCR data
                          for k in ['numberOfPosts', 'deadline', 'qualificationLevel']:
                              if ocr_job.get(k) and not j.get(k):
                                  j[k] = ocr_job[k]
                          break
          
          # Preserve applied_ids array
          data['jobListings'] = existing_jobs
          data['sections']['applied'] = list(applied_ids)
          data['sections']['other'] = list(other_ids)
          data['transparencyInfo']['mergedOCR'] = merged_count
          
          # Write back
          pathlib.Path('data.json').write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding='utf-8')
          print(f"[OCR] Merged {merged_count} new OCR jobs without losing applied data", file=sys.stderr)
          PYTHON_SCRIPT

      - name: Run QC checks ONLY (no duplicate qc_and_learn)
        run: |
          # FIX P2-C-010: Only run qc_checks for validation, NOT qc_and_learn
          # qc_and_learn already ran in scraper.yml
          python3 qc_checks.py
          echo "✓ QC checks complete"

      - name: Generate health report
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime
          
          data = json.load(open('data.json'))
          
          health = {
              'ok': True,
              'totalListings': len(data.get('jobListings', [])),
              'archivedCount': len(data.get('archivedListings', [])),
              'appliedCount': len(data.get('sections', {}).get('applied', [])),
              'lastUpdated': datetime.utcnow().isoformat() + 'Z',
              'source': 'github-actions-ocr'
          }
          
          with open('health.json', 'w') as f:
              json.dump(health, f, indent=2)
          
          print(f"✓ Health report: {health['totalListings']} active listings")
          PYTHON_SCRIPT

      - name: Pull before push (prevent conflicts)
        run: |
          git config user.name "OCR Bot"
          git config user.email "ocr-bot@users.noreply.github.com"
          
          # FIX P2-C-005: Add git pull --rebase before push
          git pull --rebase origin main || true

      - name: Commit and push
        run: |
          git add data.json health.json 2>/dev/null || true
          
          if git diff --cached --quiet; then
            echo "ℹ No new data to commit (OCR found no new jobs)"
          else
            git commit -m "chore: OCR processing complete $(date -u +'%Y-%m-%dT%H:%M:%SZ')" \
              -m "Mode: ${{ steps.payload.outputs.mode }}" \
              -m "Processed by: github-actions-ocr"
            
            echo "Pushing to main..."
            git push origin main
            echo "✓ Committed and pushed OCR results"
          fi

      - name: Cleanup
        if: always()
        run: |
          # FIX P2-H-002: Clean cache to prevent duplicate processing
          rm -rf .cache tmp/download_log.json
          echo "✓ Cache cleaned"

      - name: Workflow summary
        if: always()
        run: |
          echo "=== OCR Workflow Summary ==="
          echo "Mode: ${{ steps.payload.outputs.mode }}"
          echo "Timestamp: $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          echo "Status: ${{ job.status }}"
          
          if [ -f data.json ]; then
            python3 -c "import json; d=json.load(open('data.json')); print(f'Listings: {len(d.get(\"jobListings\", []))}')"
          fi
